{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:43:25.477658Z",
     "start_time": "2021-02-08T13:43:22.537571Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from RandomTesting.__RandomTesting__ import Random_Testing, calculate_extract_loss\n",
    "from tqdm import tqdm\n",
    "from sklearn import neighbors\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "tf.config.optimizer.set_jit(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# AIR_Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T06:33:42.428Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2020/12/03 OK\n",
    "air1718 = pd.read_csv('air-201718.csv')\n",
    "air1718_arr = air1718[7075495:]\n",
    "air19 = pd.read_csv('air-2019.csv')\n",
    "air171819_arr = np.concatenate([air1718_arr,air19])\n",
    "\n",
    "first_lst = []\n",
    "count=0\n",
    "for data_nb in tqdm(range(len(air171819_arr))):\n",
    "    if (air171819_arr[data_nb][0][-8:] == '08:00:00') or (air171819_arr[data_nb][0][-8:] == '20:00:00'):\n",
    "        count+=1\n",
    "        first_lst.append(air171819_arr[data_nb])\n",
    "print(np.array(first_lst).shape)\n",
    "df = pd.DataFrame(data=np.array(first_lst),columns=['dt',\"lat\", \"lon\",'pm25','pm10','temperature','humidity'])\n",
    "df.to_csv('AIR_Training.csv',index=False)\n",
    "\n",
    "lat_min = 21.87; lat_max = 25.35\n",
    "lon_min = 120; lon_max = 122.04\n",
    "pm25_max = 300\n",
    "\n",
    "\n",
    "df = pd.read_csv('AIR_Training.csv')\n",
    "df_lat_ok = df[(df['lat'] > lat_min)][df[(df['lat'] > lat_min)]['lat'] < lat_max]\n",
    "df_lat_lon_ok = df_lat_ok[(df_lat_ok['lon'] > lon_min)][df_lat_ok[(df_lat_ok['lon'] > lon_min)]['lon'] < lon_max]\n",
    "df_finall = df_lat_lon_ok[df_lat_lon_ok['pm25'] < pm25_max]\n",
    "\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "del mou['Unnamed: 0']\n",
    "\n",
    "dts=df_finall.dt.unique()\n",
    "\n",
    "air_kriging = []\n",
    "grid_lon = np.array(np.arange(204),dtype='float64')\n",
    "grid_lat = np.array(np.arange(348),dtype='float64') \n",
    "\n",
    "for dt in tqdm(dts[:]):\n",
    "    one_dt=df_finall[df_finall.dt==dt]\n",
    "\n",
    "    x_train = one_dt[['lat', 'lon']]\n",
    "    y_train = one_dt.iloc[:, 3:4] #pm2.5\n",
    "    print(len(x_train))\n",
    "    matrix_lon_list = [];matrix_lat_list = []; value_list=[]\n",
    "    for i in range(len(x_train)):\n",
    "        matrix_lon = int(round((np.array(x_train)[i][1]-120)/((122.04-120)/204),0))\n",
    "        matrix_lat = int(round((np.array(x_train)[i][0]-21.87)/((25.35-21.87)/348),0))\n",
    "        matrix_lon_list.append(matrix_lon)\n",
    "        matrix_lat_list.append(matrix_lat)\n",
    "        value_list.append(np.array(y_train)[i][0])\n",
    "\n",
    "    matrix_lon_array = np.concatenate([np.array(matrix_lon_list), mou['matrix_lon'].values])\n",
    "    matrix_lat_array = np.concatenate([np.array(matrix_lat_list), mou['matrix_lat'].values])\n",
    "    train_x = np.array(np.vstack((matrix_lon_array,matrix_lat_array)).T)\n",
    "    \n",
    "    mon_value = np.zeros_like(mou['matrix_lon'].values)\n",
    "    train_y = np.concatenate([np.array(value_list),mon_value])\n",
    "\n",
    "    OK = OrdinaryKriging(train_x[:,0], train_x[:,1], train_y, variogram_model='linear', weight = True,\n",
    "                         exact_values = False, verbose=False)\n",
    "\n",
    "    values, ss1 = OK.execute('grid', grid_lon, grid_lat)\n",
    "    air_kriging.append(values)\n",
    "\n",
    "    plt.imshow(values,origin='lower')\n",
    "    plt.title(str(dt))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    del matrix_lon_array; del matrix_lat_array; del train_x; del train_y; del OK; del values; del ss1        \n",
    "\n",
    "print(len(air_kriging))\n",
    "\n",
    "# find error by Manual inspection\n",
    "air_kriging = np.delete(air_kriging,[19,20,28,57,77,170,193,513,514,551,623,634,651,688,1215,\n",
    "                                     1223,1252,1254,1266,1267,1268,1271,1284,1303,1308,1353,\n",
    "                                     1359,1371,1375,1445],axis=0)\n",
    "\n",
    "np.save('AIR_DomainB_Training',np.reshape(air_kriging,(1597,348,204,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T06:33:42.429Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2020/12/03 OK\n",
    "\n",
    "air20_arr = np.array(pd.read_csv('air-2020.csv'))\n",
    "first_lst = []\n",
    "count=0\n",
    "for data_nb in tqdm(range(len(air20_arr))):\n",
    "    if (air20_arr[data_nb][0][-8:] == '08:00:00') or (air20_arr[data_nb][0][-8:] == '20:00:00'):\n",
    "        count+=1\n",
    "        first_lst.append(air20_arr[data_nb])\n",
    "print(np.array(first_lst).shape)\n",
    "df = pd.DataFrame(data=np.array(first_lst),columns=['dt',\"lat\", \"lon\",'pm25','pm10','temperature','humidity'])\n",
    "df.to_csv('AIR_Testing.csv',index=False)\n",
    "\n",
    "\n",
    "lat_min = 21.87; lat_max = 25.35\n",
    "lon_min = 120; lon_max = 122.04\n",
    "pm25_max = 300\n",
    "\n",
    "df = pd.read_csv('AIR_Testing.csv')\n",
    "df_lat_ok = df[(df['lat'] > lat_min)][df[(df['lat'] > lat_min)]['lat'] < lat_max]\n",
    "df_lat_lon_ok = df_lat_ok[(df_lat_ok['lon'] > lon_min)][df_lat_ok[(df_lat_ok['lon'] > lon_min)]['lon'] < lon_max]\n",
    "df_finall = df_lat_lon_ok[df_lat_lon_ok['pm25'] < pm25_max]\n",
    "\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "del mou['Unnamed: 0']\n",
    "\n",
    "dts=df_finall.dt.unique()\n",
    "\n",
    "air_kriging = []\n",
    "grid_lon = np.array(np.arange(204),dtype='float64')\n",
    "grid_lat = np.array(np.arange(348),dtype='float64') \n",
    "\n",
    "for dt in tqdm(dts[:]):\n",
    "    one_dt=df_finall[df_finall.dt==dt]\n",
    "\n",
    "    x_train = one_dt[['lat', 'lon']]\n",
    "    y_train = one_dt.iloc[:, 3:4] #pm2.5\n",
    "    print(len(x_train))\n",
    "    matrix_lon_list = [];matrix_lat_list = []; value_list=[]\n",
    "    for i in range(len(x_train)):\n",
    "        matrix_lon = int(round((np.array(x_train)[i][1]-120)/((122.04-120)/204),0))\n",
    "        matrix_lat = int(round((np.array(x_train)[i][0]-21.87)/((25.35-21.87)/348),0))\n",
    "        matrix_lon_list.append(matrix_lon)\n",
    "        matrix_lat_list.append(matrix_lat)\n",
    "        value_list.append(np.array(y_train)[i][0])\n",
    "\n",
    "    matrix_lon_array = np.concatenate([np.array(matrix_lon_list), mou['matrix_lon'].values])\n",
    "    matrix_lat_array = np.concatenate([np.array(matrix_lat_list), mou['matrix_lat'].values])\n",
    "    train_x = np.array(np.vstack((matrix_lon_array,matrix_lat_array)).T)\n",
    "    \n",
    "    mon_value = np.zeros_like(mou['matrix_lon'].values)\n",
    "    train_y = np.concatenate([np.array(value_list),mon_value])\n",
    "\n",
    "    OK = OrdinaryKriging(train_x[:,0], train_x[:,1], train_y, variogram_model='linear', weight = True,\n",
    "                         exact_values = False, verbose=False)\n",
    "\n",
    "    values, ss1 = OK.execute('grid', grid_lon, grid_lat)\n",
    "    air_kriging.append(values)\n",
    "\n",
    "    plt.imshow(values,origin='lower')\n",
    "    plt.title(str(dt))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    del matrix_lon_array; del matrix_lat_array; del train_x; del train_y; del OK; del values; del ss1        \n",
    "\n",
    "print(len(air_kriging))\n",
    "\n",
    "# find error by Manual inspection\n",
    "air_kriging = np.delete(air_kriging,[161,162,164,206,210,236,260,272,280,\n",
    "                                     281,282,288,289,291,292,298,304,312,\n",
    "                                     315,318,329,332,346,356],axis=0)\n",
    "\n",
    "np.save('AIR_DomainB_Testing',np.reshape(air_kriging,(338,348,204,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T04:47:13.915714Z",
     "start_time": "2021-01-31T04:46:59.498923Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2020/12/03 OK\n",
    "\n",
    "AIR_Training = np.load('AIR_DomainB_Training.npy')\n",
    "AIR_Testing = np.load('AIR_DomainB_Testing.npy')\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "AIR73_Training = np.zeros_like(AIR_Training)\n",
    "AIR73_Testing = np.zeros_like(AIR_Testing)\n",
    "\n",
    "for lat, lon in zip(sta_lat, sta_lon):\n",
    "    AIR73_Training[:, lat, lon, :] = AIR_Training[:, lat, lon, :]\n",
    "    AIR73_Testing[:, lat, lon, :] = AIR_Testing[:, lat, lon, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T04:40:29.272351Z",
     "start_time": "2020-12-31T04:37:40.049584Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "mou_lat = [mou['matrix_lat'][0], mou['matrix_lat'][71], mou['matrix_lat'][92]] #玉山, 馬比杉山, 北大武山\n",
    "mou_lon = [mou['matrix_lon'][0], mou['matrix_lon'][71], mou['matrix_lon'][92]]\n",
    "total_result = []\n",
    "test_x = ([[i,j] for i in range(348) for j in range(204)])\n",
    "\n",
    "input_data = AIR73_Training\n",
    "\n",
    "\n",
    "for i in tqdm(range(input_data.shape[0])):\n",
    "    value_list, result = ([] for _ in range(2))\n",
    "      \n",
    "    for lat, lon in zip(sta_lat, sta_lon): value_list.append(input_data[i,lat,lon,0])\n",
    "\n",
    "        \n",
    "    lat_all_mou = np.concatenate([sta_lat, mou_lat])\n",
    "    lon_all_mou = np.concatenate([sta_lon, mou_lon])\n",
    "    train_x = np.vstack((lat_all_mou,lon_all_mou)).T\n",
    "    \n",
    "    mon_value = np.zeros_like(mou_lat)\n",
    "    value_list_mou = np.concatenate([value_list,mon_value])\n",
    "    train_y = value_list_mou\n",
    "\n",
    "    knn = neighbors.KNeighborsRegressor(5, weights = 'distance') \n",
    "    knn_fit = knn.fit(train_x, train_y)\n",
    "    test_y = knn_fit.predict(test_x)\n",
    "    for k in range(348):result.append(test_y[k*204:(k+1)*204])   \n",
    "    total_result.append(result)\n",
    "\n",
    "AIR73_train_fill = total_result\n",
    "np.save('AIR_DomainA_Training_K5',np.reshape(np.array(AIR73_train_fill),(1597,348,204,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T05:14:10.537301Z",
     "start_time": "2020-12-31T05:13:36.124531Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "mou_lat = [mou['matrix_lat'][0], mou['matrix_lat'][71], mou['matrix_lat'][92]] #玉山, 馬比杉山, 北大武山\n",
    "mou_lon = [mou['matrix_lon'][0], mou['matrix_lon'][71], mou['matrix_lon'][92]]\n",
    "total_result = []\n",
    "test_x = ([[i,j] for i in range(348) for j in range(204)])\n",
    "\n",
    "input_data = AIR73_Testing\n",
    "\n",
    "\n",
    "for i in tqdm(range(input_data.shape[0])):\n",
    "    value_list, result = ([] for _ in range(2))\n",
    "\n",
    "    for lat, lon in zip(sta_lat, sta_lon): value_list.append(input_data[i,lat,lon,0])\n",
    "    \n",
    "    lat_all_mou = np.concatenate([sta_lat, mou_lat])\n",
    "    lon_all_mou = np.concatenate([sta_lon, mou_lon])\n",
    "    train_x = np.vstack((lat_all_mou,lon_all_mou)).T\n",
    "    \n",
    "    mon_value = np.zeros_like(mou_lat)\n",
    "    value_list_mou = np.concatenate([value_list,mon_value])\n",
    "    train_y = value_list_mou\n",
    "\n",
    "    knn = neighbors.KNeighborsRegressor(5, weights = 'distance') \n",
    "    knn_fit = knn.fit(train_x, train_y)\n",
    "    test_y = knn_fit.predict(test_x)\n",
    "    for k in range(348):result.append(test_y[k*204:(k+1)*204])   \n",
    "    total_result.append(result)\n",
    "\n",
    "AIR73_train_fill = total_result\n",
    "np.save('AIR_DomainA_Testing_K5',np.reshape(np.array(AIR73_train_fill),(338,348,204,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SAT_Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-25T11:06:43.626041Z",
     "start_time": "2020-12-25T11:06:18.266027Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2020/12/03 OK\n",
    "\n",
    "arrange_pd_training = pd.read_csv('AIR_Training.csv')\n",
    "SAT_time_training = np.load('Satellite_29X17_data/SAT_pm25/an_training_time.npy',allow_pickle=True)\n",
    "SAT_training = np.load('SAT_interlinear_Training_348X204_PM25.npy',mmap_mode='r')\n",
    "\n",
    "l1 = list(np.array(SAT_time_training,dtype='str'))\n",
    "l2 = arrange_pd_training.dt.unique()\n",
    "l3 = [x for x in l1 if x not in l2]\n",
    "print(l3)\n",
    "align_SAT = []\n",
    "\n",
    "for i in range(len(l1)):\n",
    "    if l1[i] in l3:\n",
    "        print(i)\n",
    "    else:\n",
    "        align_SAT.append(SAT_training[i])\n",
    "print('total:',len(align_SAT))\n",
    "\n",
    "Sat_train_del = np.delete(np.array(align_SAT),[19,20,28,57,77,170,193,513,514,551,623,634,651,688,1215,\n",
    "                                               1223,1252,1254,1266,1267,1268,1271,1284,1303,1308,1353,\n",
    "                                               1359,1371,1375,1445],axis=0)\n",
    "print(Sat_train_del.shape)\n",
    "np.save('SAT_DomainB_Training.npy',Sat_train_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-25T10:02:23.959577Z",
     "start_time": "2020-12-25T10:02:16.458830Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2020/12/03 OK\n",
    "arrange_pd = pd.read_csv('AIR_Testing.csv')\n",
    "SAT_time_testing = np.load('Satellite_29X17_data/SAT_pm25/an_testing_time.npy',allow_pickle=True)\n",
    "SAT_testing = np.load('SAT_interlinear_Testing_348X204_PM25.npy',mmap_mode='r')\n",
    "\n",
    "l1 = list(np.array(SAT_time_testing,dtype='str'))\n",
    "l2 = arrange_pd.dt.unique()\n",
    "\n",
    "l3 = [x for x in l1 if x not in l2]\n",
    "print(l3)\n",
    "\n",
    "align_SAT = []\n",
    "\n",
    "for i in range(len(l1)):\n",
    "    if l1[i] in l3:\n",
    "        print(i)\n",
    "    else:\n",
    "        align_SAT.append(SAT_testing[i])\n",
    "        \n",
    "Sat_test_del = np.delete(align_SAT,[161,162,164,206,210,236,260,272,280,\n",
    "                                     281,282,288,289,291,292,298,304,312,\n",
    "                                     315,318,329,332,346,356],axis=0)\n",
    "print('total:',len(Sat_test_del))\n",
    "# np.save('SAT_DomainB_Testing',Sat_test_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T04:48:09.699324Z",
     "start_time": "2021-01-31T04:47:48.985154Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2020/12/03 OK\n",
    "\n",
    "SAT_Training = np.load('SAT_DomainB_Training.npy')\n",
    "SAT_Testing = np.load('SAT_DomainB_Testing.npy')\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "SAT73_Training = np.zeros_like(SAT_Training)\n",
    "SAT73_Testing = np.zeros_like(SAT_Testing)\n",
    "\n",
    "for lat, lon in zip(sta_lat, sta_lon):\n",
    "    SAT73_Training[:, lat, lon, :] = SAT_Training[:, lat, lon, :]\n",
    "    SAT73_Testing[:, lat, lon, :] = SAT_Testing[:, lat, lon, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-31T05:16:27.568Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "mou_lat = [mou['matrix_lat'][0], mou['matrix_lat'][71], mou['matrix_lat'][92]] #玉山, 馬比杉山, 北大武山\n",
    "mou_lon = [mou['matrix_lon'][0], mou['matrix_lon'][71], mou['matrix_lon'][92]]\n",
    "total_result = []\n",
    "test_x = ([[i,j] for i in range(348) for j in range(204)])\n",
    "\n",
    "input_data = SAT73_Training\n",
    "\n",
    "\n",
    "for i in tqdm(range(input_data.shape[0])):\n",
    "    value_list, result = ([] for _ in range(2))\n",
    "    \n",
    "    for lat, lon in zip(sta_lat, sta_lon): value_list.append(input_data[i,lat,lon,0])\n",
    "\n",
    "    lat_all_mou = np.concatenate([sta_lat, mou_lat])\n",
    "    lon_all_mou = np.concatenate([sta_lon, mou_lon])\n",
    "    train_x = np.vstack((lat_all_mou,lon_all_mou)).T\n",
    "    \n",
    "    mon_value = np.zeros_like(mou_lat)\n",
    "    value_list_mou = np.concatenate([value_list,mon_value])\n",
    "    train_y = value_list_mou\n",
    "    \n",
    "    knn = neighbors.KNeighborsRegressor(5, weights = 'distance')    \n",
    "    \n",
    "    knn_fit = knn.fit(train_x, train_y)\n",
    "    test_y = knn_fit.predict(test_x)\n",
    "    for k in range(348):result.append(test_y[k*204:(k+1)*204])\n",
    "        \n",
    "    total_result.append(result)\n",
    "\n",
    "Sat73_train_fill = total_result\n",
    "np.save('SAT_DomainA_Training_K5',np.reshape(np.array(Sat73_train_fill),(1597,348,204,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T05:13:33.050232Z",
     "start_time": "2020-12-31T04:38:27.215Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "mou_lat = [mou['matrix_lat'][0], mou['matrix_lat'][71], mou['matrix_lat'][92]] #玉山, 馬比杉山, 北大武山\n",
    "mou_lon = [mou['matrix_lon'][0], mou['matrix_lon'][71], mou['matrix_lon'][92]]\n",
    "total_result = []\n",
    "test_x = ([[i,j] for i in range(348) for j in range(204)])\n",
    "\n",
    "input_data = SAT73_Testing\n",
    "\n",
    "\n",
    "for i in tqdm(range(input_data.shape[0])):\n",
    "    value_list, result = ([] for _ in range(2))\n",
    "    \n",
    "    for lat, lon in zip(sta_lat, sta_lon): value_list.append(input_data[i,lat,lon,0])\n",
    "\n",
    "    lat_all_mou = np.concatenate([sta_lat, mou_lat])\n",
    "    lon_all_mou = np.concatenate([sta_lon, mou_lon])\n",
    "    train_x = np.vstack((lat_all_mou,lon_all_mou)).T\n",
    "    \n",
    "    mon_value = np.zeros_like(mou_lat)\n",
    "    value_list_mou = np.concatenate([value_list,mon_value])\n",
    "    train_y = value_list_mou\n",
    "    \n",
    "    knn = neighbors.KNeighborsRegressor(5, weights = 'distance')    \n",
    "    \n",
    "    knn_fit = knn.fit(train_x, train_y)\n",
    "    test_y = knn_fit.predict(test_x)\n",
    "    for k in range(348):result.append(test_y[k*204:(k+1)*204])\n",
    "        \n",
    "    total_result.append(result)\n",
    "\n",
    "Sat73_train_fill = total_result\n",
    "np.save('SAT_DomainA_Testing_K5',np.reshape(np.array(Sat73_train_fill),(338,348,204,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPA_Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:16:40.109351Z",
     "start_time": "2021-02-01T18:29:48.849336Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "\n",
    "arrange_pd_training = pd.read_csv('AIR_Training.csv')\n",
    "SAT_time_training = np.load('Satellite_29X17_data/SAT_pm25/an_training_time.npy',allow_pickle=True)\n",
    "EPA_training = np.load('EPA73_Training_348X204_PM25.npy',mmap_mode='r')\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "del mou['Unnamed: 0']\n",
    "\n",
    "l1 = list(np.array(SAT_time_training,dtype='str'))\n",
    "l2 = arrange_pd_training.dt.unique()\n",
    "l3 = [x for x in l1 if x not in l2]\n",
    "print(l3)\n",
    "align_EPA = []\n",
    "\n",
    "for i in range(len(l1)):\n",
    "    if l1[i] in l3:\n",
    "        print(i)\n",
    "    else:\n",
    "        align_EPA.append(EPA_training[i])\n",
    "print('total:',len(align_EPA))\n",
    "\n",
    "EPA_train_del = np.delete(np.array(align_EPA),[19,20,28,57,77,170,193,513,514,551,623,634,651,688,1215,\n",
    "                                               1223,1252,1254,1266,1267,1268,1271,1284,1303,1308,1353,\n",
    "                                               1359,1371,1375,1445],axis=0)\n",
    "print(EPA_train_del.shape)\n",
    "\n",
    "\n",
    "# 2020/12/03 OK\n",
    "\n",
    "EPA_kriging = []\n",
    "grid_lon = np.array(np.arange(204),dtype='float64')\n",
    "grid_lat = np.array(np.arange(348),dtype='float64') \n",
    "\n",
    "input_data = EPA_train_del\n",
    "\n",
    "for i in tqdm(range(input_data.shape[0])):\n",
    "    value_list, result = ([] for _ in range(2))\n",
    "    for lat, lon in zip(sta_lat, sta_lon): value_list.append(input_data[i,lat,lon,0])\n",
    "\n",
    "    matrix_lon_array = np.concatenate([np.array(sta_lon), mou['matrix_lon'].values])\n",
    "    matrix_lat_array = np.concatenate([np.array(sta_lat), mou['matrix_lat'].values])\n",
    "    train_x = np.array(np.vstack((matrix_lon_array,matrix_lat_array)).T)\n",
    "    \n",
    "    mon_value = np.zeros_like(mou['matrix_lon'].values)\n",
    "    train_y = np.concatenate([np.array(value_list),mon_value])\n",
    "\n",
    "    OK = OrdinaryKriging(train_x[:,0], train_x[:,1], train_y, variogram_model='linear', weight = True,\n",
    "                         exact_values = False, verbose=False)\n",
    "\n",
    "    values, ss1 = OK.execute('grid', grid_lon, grid_lat)\n",
    "    EPA_kriging.append(values)\n",
    "np.save('EPA_DomainB_Training',np.reshape(EPA_kriging,(1597,348,204,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T02:42:52.530995Z",
     "start_time": "2021-02-01T21:16:48.555107Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "EPA_Testing = np.load('EPA73_Testing_348X204_PM25.npy',mmap_mode='r')\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "del mou['Unnamed: 0']\n",
    "EPA_kriging = []\n",
    "grid_lon = np.array(np.arange(204),dtype='float64')\n",
    "grid_lat = np.array(np.arange(348),dtype='float64') \n",
    "\n",
    "input_data = EPA_Testing\n",
    "\n",
    "for i in tqdm(range(input_data.shape[0])):\n",
    "    value_list, result = ([] for _ in range(2))\n",
    "    \n",
    "\n",
    "    for lat, lon in zip(sta_lat, sta_lon): value_list.append(input_data[i,lat,lon,0])\n",
    "\n",
    "    matrix_lon_array = np.concatenate([np.array(sta_lon), mou['matrix_lon'].values])\n",
    "    matrix_lat_array = np.concatenate([np.array(sta_lat), mou['matrix_lat'].values])\n",
    "    train_x = np.array(np.vstack((matrix_lon_array,matrix_lat_array)).T)\n",
    "    \n",
    "    mon_value = np.zeros_like(mou['matrix_lon'].values)\n",
    "    train_y = np.concatenate([np.array(value_list),mon_value])\n",
    "    \n",
    "    OK = OrdinaryKriging(train_x[:,0], train_x[:,1], train_y, variogram_model='linear', weight = True,\n",
    "                         exact_values = False, verbose=False)\n",
    "\n",
    "    values, ss1 = OK.execute('grid', grid_lon, grid_lat)\n",
    "    EPA_kriging.append(values)\n",
    "np.save('EPA_DomainB_Testing',np.reshape(EPA_kriging,(3000,348,204,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T02:43:57.790940Z",
     "start_time": "2021-02-02T02:42:59.980329Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2020/12/03 OK\n",
    "\n",
    "EPA_Training = np.load('EPA_DomainB_Training.npy')\n",
    "EPA_Testing = np.load('EPA_DomainB_Testing.npy')\n",
    "EPA73_Training = np.zeros_like(EPA_Training)\n",
    "EPA73_Testing = np.zeros_like(EPA_Testing)\n",
    "\n",
    "for lat, lon in zip(sta_lat, sta_lon):\n",
    "    EPA73_Training[:, lat, lon, :] = EPA_Training[:, lat, lon, :]\n",
    "    EPA73_Testing[:, lat, lon, :] = EPA_Testing[:, lat, lon, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T02:46:45.309751Z",
     "start_time": "2021-02-02T02:44:04.932655Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "mou_lat = [mou['matrix_lat'][0], mou['matrix_lat'][71], mou['matrix_lat'][92]] #玉山, 馬比杉山, 北大武山\n",
    "mou_lon = [mou['matrix_lon'][0], mou['matrix_lon'][71], mou['matrix_lon'][92]]\n",
    "total_result = []\n",
    "test_x = ([[i,j] for i in range(348) for j in range(204)])\n",
    "\n",
    "input_data = EPA73_Training\n",
    "\n",
    "for i in tqdm(range(input_data.shape[0])):\n",
    "    value_list, result = ([] for _ in range(2))\n",
    "    \n",
    "    for lat, lon in zip(sta_lat, sta_lon): value_list.append(input_data[i,lat,lon,0])\n",
    "\n",
    "    lat_all_mou = np.concatenate([sta_lat, mou_lat])\n",
    "    lon_all_mou = np.concatenate([sta_lon, mou_lon])\n",
    "    train_x = np.vstack((lat_all_mou,lon_all_mou)).T\n",
    "    \n",
    "    mon_value = np.zeros_like(mou_lat)\n",
    "    value_list_mou = np.concatenate([value_list,mon_value])\n",
    "    train_y = value_list_mou\n",
    "    \n",
    "    knn = neighbors.KNeighborsRegressor(3, weights = 'distance')    \n",
    "    \n",
    "    knn_fit = knn.fit(train_x, train_y)\n",
    "    test_y = knn_fit.predict(test_x)\n",
    "    for k in range(348):result.append(test_y[k*204:(k+1)*204])\n",
    "        \n",
    "    total_result.append(result)\n",
    "\n",
    "EPA73_train_fill = total_result\n",
    "np.save('EPA_DomainA_Training',np.reshape(EPA73_train_fill,(1597,348,204,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T03:24:05.827483Z",
     "start_time": "2021-02-02T03:17:48.295345Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "mou_lat = [mou['matrix_lat'][0], mou['matrix_lat'][71], mou['matrix_lat'][92]] #玉山, 馬比杉山, 北大武山\n",
    "mou_lon = [mou['matrix_lon'][0], mou['matrix_lon'][71], mou['matrix_lon'][92]]\n",
    "total_result = []\n",
    "test_x = ([[i,j] for i in range(348) for j in range(204)])\n",
    "\n",
    "input_data = EPA73_Testing\n",
    "\n",
    "for i in tqdm(range(input_data.shape[0])):\n",
    "    value_list, result = ([] for _ in range(2))\n",
    "\n",
    "    for lat, lon in zip(sta_lat, sta_lon): value_list.append(input_data[i,lat,lon,0])\n",
    "\n",
    "    lat_all_mou = np.concatenate([sta_lat, mou_lat])\n",
    "    lon_all_mou = np.concatenate([sta_lon, mou_lon])\n",
    "    train_x = np.vstack((lat_all_mou,lon_all_mou)).T\n",
    "    \n",
    "    mon_value = np.zeros_like(mou_lat)\n",
    "    value_list_mou = np.concatenate([value_list,mon_value])\n",
    "    train_y = value_list_mou\n",
    "    \n",
    "    knn = neighbors.KNeighborsRegressor(3, weights = 'distance')    \n",
    "    \n",
    "    knn_fit = knn.fit(train_x, train_y)\n",
    "    test_y = knn_fit.predict(test_x)\n",
    "    for k in range(348):result.append(test_y[k*204:(k+1)*204])\n",
    "        \n",
    "    total_result.append(result)\n",
    "\n",
    "EPA73_test_fill = total_result\n",
    "np.save('EPA_DomainA_Testing',np.reshape(EPA73_test_fill,(3000,348,204,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# KNN Random Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-08T04:58:11.085Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "EPA73_test_pm25 = np.load('EPA73_Testing_348X204_PM25.npy', mmap_mode='r')\n",
    "station_coordinate = pd.read_csv('RandomTesting/station_coordinate')\n",
    "ex5_lst = np.load('RandomTesting/extract5_list.npy')\n",
    "ex10_lst = np.load('RandomTesting/extract10_list.npy')\n",
    "ex15_lst = np.load('RandomTesting/extract15_list.npy')\n",
    "ex20_lst = np.load('RandomTesting/extract20_list.npy')\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "mou_lat = [mou['matrix_lat'][0], mou['matrix_lat'][71], mou['matrix_lat'][92]] #玉山, 馬比杉山, 北大武山\n",
    "mou_lon = [mou['matrix_lon'][0], mou['matrix_lon'][71], mou['matrix_lon'][92]]\n",
    "\n",
    "K_nb = 5\n",
    "for extract_nb in range(3,4):\n",
    "    total_mse, EPA73_test_fill_ex20 = ([] for _ in range(2))\n",
    "    for qq in range(30):\n",
    "        matrix20,lats_ex20,lons_ex20=Random_Testing(extract_nb, qq, station_coordinate, EPA73_test_pm25, \n",
    "                                                    ex5_lst, ex10_lst, ex15_lst, ex20_lst)\n",
    "\n",
    "        total_result = []\n",
    "        test_x = ([[i,j] for i in range(348) for j in range(204)])\n",
    "        input_data = matrix20\n",
    "        for i in tqdm(range(input_data.shape[0])):\n",
    "            value_list, result = ([] for _ in range(2))\n",
    "\n",
    "            for lat, lon in zip(sta_lat, sta_lon): value_list.append(input_data[i,lat,lon,0])\n",
    "\n",
    "            matrix_lon_array = np.concatenate([np.array(sta_lon),mou_lon])\n",
    "            matrix_lat_array = np.concatenate([np.array(sta_lat),mou_lat])\n",
    "\n",
    "            train_x = np.array(np.vstack((matrix_lat_array,matrix_lon_array)).T)\n",
    "\n",
    "            mon_value = np.zeros_like(mou_lat)\n",
    "            train_y = np.concatenate([np.array(value_list),mon_value])        \n",
    "\n",
    "            knn = neighbors.KNeighborsRegressor(K_nb, weights = 'distance')\n",
    "\n",
    "            knn_fit = knn.fit(train_x, train_y)\n",
    "            test_y = knn_fit.predict(test_x)\n",
    "            for k in range(348): result.append(test_y[k*204:(k+1)*204])\n",
    "            total_result.append(result)\n",
    "\n",
    "            del result, test_y\n",
    "            gc.collect()\n",
    "#         mse,mae,mape = calculate_extract_loss(np.array(total_result),EPA73_test_pm25[:,:,:,0],lats_ex20,lons_ex20)\n",
    "#         total_mse.append(mse)\n",
    "#         print(mse)        \n",
    "        EPA73_test_fill_ex20.append(total_result)\n",
    "        del input_data, matrix20, total_result\n",
    "#     print(np.array(total_mse).mean())        \n",
    "    np.save('EPA73_testing_KNN'+str(K_nb)+'fill_ex'+str(extract_nb*5),EPA73_test_fill_ex20)\n",
    "    print('*'*50)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Kriging Random Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T11:58:19.410Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2021/02/02 OK\n",
    "\n",
    "sta_lat=[9,61,66,70,70,74,74,76,77,81,81,82,87,89,89,\n",
    "         102,112,118,118,125,144,160,160,169,185,185,\n",
    "         189,189,189,205,206,210,211,220,223,227,229,\n",
    "         230,236,239,252,270,277,283,288,288,294,300,\n",
    "         304,309,309,311,312,313,315,315,316,317,317,\n",
    "         318,319,320,320,320,320,320,321,321,324,326,\n",
    "         330,331,332]\n",
    "\n",
    "sta_lon=[79,42,57,34,43,31,32,36,29,30,49,34,33,31,116,\n",
    "         54,21,22,117,30,32,25,45,35,21,55,26,35,68,69,\n",
    "         41,97,160,55,68,47,65,62,57,75,76,83,180,90,109,\n",
    "         175,98,122,104,121,123,154,146,131,146,152,153,\n",
    "         109,144,151,158,121,149,152,153,165,137,150,152,\n",
    "         177,145,169,153]\n",
    "\n",
    "EPA73_test_pm25 = np.load('EPA73_Testing_348X204_PM25.npy', mmap_mode='r')\n",
    "station_coordinate = pd.read_csv('RandomTesting/station_coordinate')\n",
    "ex5_lst = np.load('RandomTesting/extract5_list.npy')\n",
    "ex10_lst = np.load('RandomTesting/extract10_list.npy')\n",
    "ex15_lst = np.load('RandomTesting/extract15_list.npy')\n",
    "ex20_lst = np.load('RandomTesting/extract20_list.npy')\n",
    "mou = pd.read_csv('mou_process_done.csv')\n",
    "mou_lat = [mou['matrix_lat'][0], mou['matrix_lat'][71], mou['matrix_lat'][92]] #玉山, 馬比杉山, 北大武山\n",
    "mou_lon = [mou['matrix_lon'][0], mou['matrix_lon'][71], mou['matrix_lon'][92]]\n",
    "\n",
    "grid_lon = np.array(np.arange(204),dtype='float64')\n",
    "grid_lat = np.array(np.arange(348),dtype='float64') \n",
    "\n",
    "total_mse = []\n",
    "EPA73_testing_Krigingfill_ex20=[]\n",
    "for qq in range(30):\n",
    "    matrix20,lats_ex20,lons_ex20=Random_Testing(2,qq,station_coordinate, EPA73_test_pm25, ex5_lst, ex10_lst, ex15_lst, ex20_lst)\n",
    "    total_result = []\n",
    "    for i in tqdm(range(matrix20.shape[0])):\n",
    "        value_list = [] \n",
    "\n",
    "        for lat, lon in zip(sta_lat, sta_lon): value_list.append(matrix20[i,lat,lon,0])\n",
    "\n",
    "        matrix_lon_array = np.concatenate([np.array(sta_lon),mou_lat])\n",
    "        matrix_lat_array = np.concatenate([np.array(sta_lat),mou_lon])\n",
    "        \n",
    "        train_x = np.array(np.vstack((matrix_lon_array,matrix_lat_array)).T)\n",
    "\n",
    "        mon_value = np.zeros_like(mou_lat)\n",
    "        train_y = np.concatenate([np.array(value_list),mon_value])         \n",
    "\n",
    "\n",
    "        OK = OrdinaryKriging(train_x[:,0], train_x[:,1], train_y, variogram_model='linear', \n",
    "                             weight = True, exact_values = False, verbose=False)\n",
    "\n",
    "        values, ss1 = OK.execute('grid', grid_lon, grid_lat)\n",
    "        total_result.append(values)\n",
    "        del values, ss1, matrix_lon_array, matrix_lat_array, train_x, train_y\n",
    "        \n",
    "    mse,mae,mape = calculate_extract_loss(np.array(total_result),EPA73_test_pm25[:,:,:,0],lats_ex20,lons_ex20)\n",
    "    del matrix20\n",
    "\n",
    "    total_mse.append(mse)\n",
    "    print(mse)        \n",
    "\n",
    "    EPA73_testing_Krigingfill_ex20.append(total_result)\n",
    "print(np.array(total_mse).mean())\n",
    "# np.save('EPA73_testing_Krigingfill_ex20',EPA73_testing_Krigingfill_ex20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
